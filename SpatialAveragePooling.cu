#include "utils.h"

#define CUDA_MAX_THREADS 1024   // this is safe, in reality 256 is our limit

/*
 * Description:
 *    this function avg-pools an input 3D tensor along dimensions 1 and 2
 *    3D input, 3D output
 */
__global__ void subsample(float *input, float *output,
                          int input_n, int input_h, int input_w,
                          int output_h, int output_w,
                          int kH, int kW, int dH, int dW)
{
  // iterators
  int xx, yy;

  // compute offsets based on thread/block ID
  int o = blockIdx.x;
  int i = o;

  int xx_start = threadIdx.x;
  int xx_end = output_w;
  int xx_step = blockDim.x;

  int yy_start = blockDim.y*blockIdx.y + threadIdx.y;
  int yy_end = output_h;
  int yy_step = blockDim.y*gridDim.y;

  // select input/output plane
  output = output + o*output_w*output_h;
  input = input + i*input_w*input_h;

  // For all output pixels...
  for(yy = yy_start; yy < yy_end; yy+=yy_step) {
    for(xx = xx_start; xx < xx_end; xx+=xx_step) {
      // Get effective pooling window size
      int hend = min(kH, input_h - yy*dH);
      int wend = min(kW, input_w - xx*dW);

      // Compute the mean of the input image...
      float *ptr_input = input + yy*dH*input_w + xx*dW;
      float *ptr_output = output + yy*output_w + xx;
      float sum = 0;
      int kx, ky;
      for(ky = 0; ky < hend; ky++) {
        for(kx = 0; kx < wend; kx++)
          sum += ptr_input[kx];
        ptr_input += input_w; // next input line
      }
      // Update output
      *ptr_output = sum/float(wend*hend);
    }
  }
}

static int cunn_SpatialAveragePooling_updateOutput(lua_State *L)
{
  THCState *state = getCutorchState(L);
  THCudaTensor *input = (THCudaTensor *)luaT_checkudata(L, 2, "torch.CudaTensor");
  int kW = luaT_getfieldcheckint(L, 1, "kW");
  int kH = luaT_getfieldcheckint(L, 1, "kH");
  int dW = luaT_getfieldcheckint(L, 1, "dW");
  int dH = luaT_getfieldcheckint(L, 1, "dH");
  bool ceil_mode = luaT_getfieldcheckboolean(L, 1, "ceil_mode");

  THCudaTensor *output = (THCudaTensor *)luaT_getfieldcheckudata(L, 1, "output", "torch.CudaTensor");
  THAssert(THCudaTensor_checkGPU(state, 2, input, output));

  float *output_data;
  float *input_data;

  luaL_argcheck(L, input->nDimension == 3 || input->nDimension == 4, 2, "3D or 4D (batch) tensor expected");

  long nOutputCols;
  long nOutputRows;

  if (input->nDimension == 3) {
    long nInputCols = input->size[2];
    long nInputRows = input->size[1];
    long nInputPlane = input->size[0];

    if(ceil_mode) {
      nOutputCols = ceil(float(nInputCols - kW) / float(dW)) + 1;
      nOutputRows = ceil(float(nInputRows - kH) / float(dH)) + 1;
    }
    else {
      nOutputCols = floor(float(nInputCols - kW) / float(dW)) + 1;
      nOutputRows = floor(float(nInputRows - kH) / float(dH)) + 1;
    }

    luaL_argcheck(L, nInputCols >= kW && nInputRows >= kH, 2, "input image smaller than kernel size");

    input = THCudaTensor_newContiguous(state, input);
    input_data = THCudaTensor_data(state, input);

    THCudaTensor_resize3d(state, output, nInputPlane, nOutputRows, nOutputCols);
    output_data = THCudaTensor_data(state, output);

    // cuda blocks & threads:
    int yblocks = (int)(16L / nInputPlane);
    yblocks = yblocks < 1 ? 1 : yblocks;
    dim3 blocks(nInputPlane,yblocks);
    dim3 threads(32,8);

    // run subsample kernel
    subsample <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (input_data, output_data,
                                     nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
                                     kH, kW, dH, dW);
  } else {
    long nInputCols = input->size[3];
    long nInputRows = input->size[2];
    long nInputPlane = input->size[1];
    long nbatch = input->size[0];

    if(ceil_mode) {
      nOutputCols = ceil(float(nInputCols - kW) / float(dW)) + 1;
      nOutputRows = ceil(float(nInputRows - kH) / float(dH)) + 1;
    }
    else {
      nOutputCols = floor(float(nInputCols - kW) / float(dW)) + 1;
      nOutputRows = floor(float(nInputRows - kH) / float(dH)) + 1;
    }

    luaL_argcheck(L, nInputCols >= kW && nInputRows >= kH, 2, "input image smaller than kernel size");

    input = THCudaTensor_newContiguous(state, input);
    input_data = THCudaTensor_data(state, input);

    THCudaTensor_resize4d(state, output, nbatch, nInputPlane, nOutputRows, nOutputCols);
    output_data = THCudaTensor_data(state, output);

    // cuda blocks & threads:
    int yblocks = (int)(16L / nInputPlane);
    yblocks = yblocks < 1 ? 1 : yblocks;
    dim3 blocks(nInputPlane*nbatch,yblocks);
    dim3 threads(32,8);

    // run subsample kernel
    subsample <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (input_data, output_data,
                                     nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
                                     kH, kW, dH, dW);
  }

  // clean
  THCudaTensor_free(state, input);

  // check for errors
  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    printf("error in SpatialAveragePooling.updateOutput: %s\n", cudaGetErrorString(err));
    THError("aborting");
  }
  return 1;
}


/*
 * Description:
 *    this function computes the gradInput from gradOutput
 */
__global__ void subgradinput(float *gradInput, float *gradOutput,
                             int input_n, int input_h, int input_w,
                             int output_h, int output_w,
                             int kH, int kW, int dH, int dW)
{
  // iterators
  int xx, yy;

  // compute offsets based on thread/block ID
  int o = blockIdx.x;
  int i = o;

  int xx_start = threadIdx.x;
  int xx_end = output_w;
  int xx_step = blockDim.x;

  int yy_start = blockDim.y*blockIdx.y + threadIdx.y;
  int yy_end = output_h;
  int yy_step = blockDim.y*gridDim.y;

  // select input/output plane
  gradOutput = gradOutput + o*output_w*output_h;
  gradInput = gradInput + i*input_w*input_h;

  // compute gradInput
  for(yy = yy_start; yy < yy_end; yy+=yy_step) {
    for(xx = xx_start; xx < xx_end; xx+=xx_step) {
      // Get effective pooling window size
      int hend = min(kH, input_h - yy*dH);
      int wend = min(kW, input_w - xx*dW);

      float *ptr_gradInput = gradInput + yy*dH*input_w + xx*dW;
      float *ptr_gradOutput = gradOutput + yy*output_w + xx;
      float z = *ptr_gradOutput;
      int kx, ky;
      for(ky = 0; ky < hend; ky++) {
        for(kx = 0; kx < wend; kx++)
          ptr_gradInput[kx] += z / float(wend*hend);
        ptr_gradInput += input_w;
      }
    }
  }
}

/*
 * Description:
 *    this function computes the gradInput from gradOutput
 *    but with an atomic accumulation. It is needed to be done so
 *    for cases of kH != dH and kW != dW
 */
__global__ void subgradinputAtomic(float *gradInput, float *gradOutput,
                                   int input_n, int input_h, int input_w,
                                   int output_h, int output_w,
                                   int kH, int kW, int dH, int dW)
{
  // iterators
  int xx, yy;

  // compute offsets based on thread/block ID
  int o = blockIdx.x;
  int i = o;

  int xx_start = threadIdx.x;
  int xx_end = output_w;
  int xx_step = blockDim.x;

  int yy_start = blockDim.y*blockIdx.y + threadIdx.y;
  int yy_end = output_h;
  int yy_step = blockDim.y*gridDim.y;

  // select input/output plane
  gradOutput = gradOutput + o*output_w*output_h;
  gradInput = gradInput + i*input_w*input_h;

  // compute gradInput
  for(yy = yy_start; yy < yy_end; yy+=yy_step) {
    for(xx = xx_start; xx < xx_end; xx+=xx_step) {
      // Get effective pooling window size
      int hend = min(kH, input_h - yy*dH);
      int wend = min(kW, input_w - xx*dW);

      float *ptr_gradInput = gradInput + yy*dH*input_w + xx*dW;
      float *ptr_gradOutput = gradOutput + yy*output_w + xx;
      float z = *ptr_gradOutput;
      int kx, ky;
      for(ky = 0; ky < hend; ky++) {
        for(kx = 0; kx < wend; kx++) {
          atomicAdd(&(ptr_gradInput[kx]), z / float(wend*hend));
        }
        ptr_gradInput += input_w;
      }
    }
  }
}


static int cunn_SpatialAveragePooling_updateGradInput(lua_State *L)
{
  THCState *state = getCutorchState(L);
  THCudaTensor *input = (THCudaTensor *)luaT_checkudata(L, 2, "torch.CudaTensor");
  THCudaTensor *gradOutput = (THCudaTensor *)luaT_checkudata(L, 3, "torch.CudaTensor");
  int kW = luaT_getfieldcheckint(L, 1, "kW");
  int kH = luaT_getfieldcheckint(L, 1, "kH");
  int dW = luaT_getfieldcheckint(L, 1, "dW");
  int dH = luaT_getfieldcheckint(L, 1, "dH");
  bool ceil_mode = luaT_getfieldcheckboolean(L, 1, "ceil_mode");

  THCudaTensor *gradInput = (THCudaTensor *)luaT_getfieldcheckudata(L, 1, "gradInput", "torch.CudaTensor");
  THAssert(THCudaTensor_checkGPU(state, 3, input, gradInput, gradOutput));

  long nOutputCols;
  long nOutputRows;

  if (input->nDimension == 3) {
    long nInputCols = input->size[2];
    long nInputRows = input->size[1];
    long nInputPlane = input->size[0];

    if(ceil_mode) {
      nOutputCols = ceil(float(nInputCols - kW) / float(dW)) + 1;
      nOutputRows = ceil(float(nInputRows - kH) / float(dH)) + 1;
    }
    else {
      nOutputCols = floor(float(nInputCols - kW) / float(dW)) + 1;
      nOutputRows = floor(float(nInputRows - kH) / float(dH)) + 1;
    }

    float *gradOutput_data = THCudaTensor_data(state, gradOutput);
    float *gradInput_data;

    THCudaTensor_resizeAs(state, gradInput, input);
    THCudaTensor_zero(state, gradInput);
    gradInput_data = THCudaTensor_data(state, gradInput);

    // cuda blocks & threads:
    int yblocks = (int)(16L / nInputPlane);
    yblocks = yblocks < 1 ? 1 : yblocks;
    dim3 blocks(nInputPlane,yblocks);
    dim3 threads(32,8);

    // run updateGradInput kernel
    if (kH == dH && kW == dW) {
      subgradinput <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (gradInput_data, gradOutput_data,
                                          nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
                                          kH, kW, dH, dW);
    } else {
      subgradinputAtomic <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (gradInput_data, gradOutput_data,
                                                nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
                                                kH, kW, dH, dW);
    }
  } else {
    long nInputCols = input->size[3];
    long nInputRows = input->size[2];
    long nInputPlane = input->size[1];
    long nbatch = input->size[0];

    if(ceil_mode) {
      nOutputCols = ceil(float(nInputCols - kW) / float(dW)) + 1;
      nOutputRows = ceil(float(nInputRows - kH) / float(dH)) + 1;
    }
    else {
      nOutputCols = floor(float(nInputCols - kW) / float(dW)) + 1;
      nOutputRows = floor(float(nInputRows - kH) / float(dH)) + 1;
    }

    float *gradOutput_data = THCudaTensor_data(state, gradOutput);
    float *gradInput_data;

    THCudaTensor_resizeAs(state, gradInput, input);
    THCudaTensor_zero(state, gradInput);
    gradInput_data = THCudaTensor_data(state, gradInput);

    // cuda blocks & threads:
    int yblocks = (int)(16L / nInputPlane);
    yblocks = yblocks < 1 ? 1 : yblocks;
    dim3 blocks(nInputPlane*nbatch,yblocks);
    dim3 threads(32,8);

    // run updateGradInput kernel
    if (kH == dH && kW == dW) {
      subgradinput <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (gradInput_data, gradOutput_data,
                                          nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
                                          kH, kW, dH, dW);
    } else {
      subgradinputAtomic <<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (gradInput_data, gradOutput_data,
                                                nInputPlane, nInputRows, nInputCols, nOutputRows, nOutputCols,
                                                kH, kW, dH, dW);
    }
  }

  // check for errors
  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    printf("error in SpatialAveragePooling.updateGradInput: %s\n", cudaGetErrorString(err));
    THError("aborting");
  }
  return 1;
}

static const struct luaL_Reg cunn_SpatialAveragePooling__ [] = {
  {"SpatialAveragePooling_updateOutput", cunn_SpatialAveragePooling_updateOutput},
  {"SpatialAveragePooling_updateGradInput", cunn_SpatialAveragePooling_updateGradInput},
  {NULL, NULL}
};

void cunn_SpatialAveragePooling_init(lua_State *L)
{
  luaT_pushmetatable(L, "torch.CudaTensor");
  luaT_registeratname(L, cunn_SpatialAveragePooling__, "nn");
  lua_pop(L,1);
}

#undef CUDA_MAX_THREADS
